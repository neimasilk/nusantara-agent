\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{array}

\title{Neuro-Symbolic Legal Reasoning with Expert-Verified Customary Law Rules: A Pilot Study on Indonesian Legal Pluralism}
\author{
Anonymous Authors\\
Project: Nusantara-Agent
}
\date{Draft v0.4 -- 2026-02-20}

\begin{document}
\maketitle

\begin{abstract}
We present \textit{Nusantara-Agent}, a neuro-symbolic framework for legal reasoning under Indonesian legal pluralism.
The core contribution is a formally encoded customary-law rule base in Answer Set Programming (ASP), constructed from expert-verified rules across three domains: Minangkabau (25 rules), Bali (34 rules), and Jawa (36 rules).
An LLM adjudication layer synthesizes symbolic outputs into final classifications.
On a benchmark of 74 dual-labeled legal scenarios (70 evaluable, 4 disputed), we compare ASP-only (keyword heuristic) against ASP+LLM (neuro-symbolic).
ASP-only achieves 58.6\% accuracy (41/70), while ASP+LLM achieves 64.3\% (45/70), Wilson 95\% CI [0.526, 0.745].
Per-label recall for ASP+LLM is A=4/6, B=23/31, C=18/31, and D=0/2.
Inter-rater agreement between two legal experts is 94.0\% on the expanded batch (50 new cases), up from 58.3\% on the initial batch after rubric refinement.
These findings demonstrate that expert-verified symbolic rules provide measurable value as an LLM reasoning anchor for plural-law classification.
\end{abstract}

\section{Introduction}
Plural legal reasoning in Indonesia requires reconciling national law and multiple customary law systems.
This setting is relevant for AI because legal outcomes depend on cross-regime conflict handling, not only document retrieval.
Our objective is to build an auditable framework that can:
\begin{enumerate}[nosep]
  \item formalize customary legal constraints,
  \item combine symbolic and neural reasoning under explicit traceability, and
  \item report disagreement and uncertainty instead of forcing brittle single-label outputs.
\end{enumerate}

\paragraph{Current scope.}
Following the 2026-02-12 pivot, this manuscript focuses on ASP rule formalization, expert verification, and pilot evaluation.
Claims about large-scale generalization are intentionally deferred.

\section{Related Work}
This work is connected to neuro-symbolic AI, legal-domain NLP, and LLM-based reasoning.
The neuro-symbolic paradigm, combining neural learning with symbolic knowledge representation, has been identified as the ``3rd wave'' of AI \cite{garcez2023neurosymbolic}.
Answer Set Programming (ASP) provides a declarative formalism for non-monotonic reasoning \cite{lifschitz2019asp}, with Clingo as the state-of-the-art solver \cite{gebser2014clingo}.
Foundational LLM and prompting work includes Transformers, few-shot scaling, chain-of-thought, and tool-using reasoning \cite{vaswani2017attention,brown2020language,wei2022cot,wang2022selfconsistency,yao2023react,yao2023tree,shinn2023reflexion}.
Recent retrieval-centered directions include classical RAG, self-reflective RAG, and hierarchical retrieval designs \cite{lewis2020rag,asai2023selfrag,gao2023ragsurvey,sarthi2024raptor,he2024gretriever}.
For legal-domain NLP, we reference benchmark and applied studies in legal judgment prediction, contract IE, legal summarization, and deployment settings \cite{guha2023legalbench,colombo2024saullm,sie-etal-2024-summarizing,liu-etal-2024-enhancing-legal,nigam-etal-2024-rethinking,xie-etal-2024-clc,mali-etal-2024-information,narendra-etal-2024-enhancing,hou-etal-2024-gaps,kwak-etal-2024-classify,taranukhin-etal-2024-empowering,tran-etal-2024-deberta,tyss-etal-2024-lexsumm,tyss-etal-2024-supporting}.
Indonesian legal pluralism, where national codified law coexists with diverse customary (\textit{adat}) systems, has been extensively studied in legal anthropology \cite{hooker1978adat,burns2004leiden}.

\section{Task Definition}
Given a legal scenario $x$, the system predicts one of four policy labels:
\begin{itemize}[nosep]
  \item \textbf{A}: mainly national-law resolution,
  \item \textbf{B}: mainly customary-law resolution,
  \item \textbf{C}: synthesis of national + customary law,
  \item \textbf{D}: clarification required.
\end{itemize}

\noindent
Predictions are compared against expert labels with explicit agreement status.

\section{System Overview}
Nusantara-Agent is rule-centric: ASP formalization of expert-verified customary law is the primary modeling contribution.
The LLM-based adjudication layer is used as a practical decision interface and is not the central novelty claim.

\subsection{Core Neuro-Symbolic Components}
\begin{itemize}[nosep]
  \item Keyword router and fact extraction for domain routing and ASP fact grounding.
  \item ASP rule engine (Clingo) for hard legal constraints and contradiction signals.
  \item Optional LLM adjudication for final label synthesis; offline fallback for deterministic operation.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lrrl}
\toprule
Domain & Expert-Verified Rules & ASP Rule File & Scope \\
\midrule
Minangkabau & 25 & \texttt{minangkabau.lp} & Matrilineal inheritance and pusako norms \\
Bali & 34 & \texttt{bali.lp} & Purusa/sentana and druwe classifications \\
Jawa & 36 & \texttt{jawa.lp} & Bilateral inheritance and gono-gini patterns \\
\midrule
Total & 95 & 3 domains & Expert-verified customary law base \\
\bottomrule
\end{tabular}
\caption{Expert-verified customary-law rules. Of 95 total, 71 are currently encoded in ASP; the remaining 24 were found to cause accuracy regression when added (see Section~\ref{sec:overspec}).}
\label{tab:rulebase}
\end{table}

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{m{2.6cm}c m{2.6cm}c m{2.6cm}c m{2.6cm}}
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Input\\Case Query}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Router +\\Fact Extraction}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering ASP Rule\\Engine}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Label Proposal\\(A/B/C/D)}}
\end{tabular}

\vspace{0.4em}
\begin{tabular}{m{3.6cm}c m{3.6cm}c m{3.6cm}}
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Optional LLM\\Adjudication}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Offline Safety Net\\(No API Mode)}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Final Report\\+ Trace}}
\end{tabular}
\caption{High-level architecture of Nusantara-Agent.}
\label{fig:architecture}
\end{figure}

\section{Dataset and Expert Protocol}
\subsection{Current Snapshot (2026-02-19)}
The benchmark dataset was constructed in two phases: an initial batch of 24 cases and an expanded batch of 50 new cases.
Each case was independently labeled by two qualified legal experts (Ahli-1: Dr.~Hendra Kusuma, S.H., M.Hum.; Ahli-2: Dr.~Indra Gunawan, S.H., M.H.).

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Item & Value \\
\midrule
Total cases in benchmark & 74 \\
Evaluable agreed cases & 70 \\
Disputed cases (excluded from accuracy) & 4 \\
Initial batch agreement (24 cases) & 58.3\% (Cohen's $\kappa$ = 0.394) \\
Expanded batch agreement (50 cases) & 94.0\% (47/50) \\
Benchmark manifest source & \texttt{data/benchmark\_manifest.json} \\
\bottomrule
\end{tabular}
\caption{Operational dataset status.}
\label{tab:status}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
Gold Label & Count & Percentage \\
\midrule
A & 6 & 8.6\% \\
B & 31 & 44.3\% \\
C & 31 & 44.3\% \\
D & 2 & 2.9\% \\
\bottomrule
\end{tabular}
\caption{Label distribution in the evaluable benchmark subset (N=70).}
\label{tab:labeldist}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Agreement Metric & Value \\
\midrule
Initial batch agreed/total & 14/24 (58.3\%) \\
Expanded batch agreed/total & 47/50 (94.0\%) \\
Adjudicated (initial disputed) & 9/10 resolved \\
Remaining disputed & 4/74 (5.4\%) \\
Accuracy reporting policy & Computed only on agreed subset \\
\bottomrule
\end{tabular}
\caption{Agreement profile across annotation phases.}
\label{tab:consensus}
\end{table}

\subsection{Rubric refinement}
Initial inter-rater agreement was moderate ($\kappa$ = 0.394), primarily due to ambiguity at the B/C boundary (pure adat vs.\ national--adat conflict).
A structured rubric with explicit boundary guidance was developed and distributed before the expanded batch.
This improved agreement to 94\%, with all 3 remaining disputes occurring at the same B/C boundary in Jawa inheritance cases.

\subsection{Adjudication policy}
For the current evaluation:
\begin{itemize}[nosep]
  \item only agreed cases are used for quantitative accuracy reporting,
  \item disputed cases are excluded and queued for third-expert tiebreak,
  \item benchmark manifest is treated as the reporting authority for snapshot counts.
\end{itemize}

\section{Metrics and Equations}
To avoid ambiguous reporting, we explicitly define pilot metrics.

\subsection{Agreement Coverage}
Let $N$ be total active cases and $N_a$ the agreed/evaluable subset.
\begin{equation}
\mathrm{ConsensusStrength} = \frac{N_a}{N}
\label{eq:consensus_strength}
\end{equation}
For the current snapshot, $(N_a, N)=(70,74)$, so:
\begin{equation}
\mathrm{ConsensusStrength} = \frac{70}{74}=0.946
\end{equation}

\subsection{Dispute Rate}
Let $N_d$ be the number of disputed cases:
\begin{equation}
\mathrm{TieRate} = \frac{N_d}{N}
\label{eq:tierate}
\end{equation}
With $N_d=4$ and $N=74$, $\mathrm{TieRate}=0.054$.

\subsection{Binomial Confidence Interval (Wilson)}
For observed accuracy $\hat{p}$ with sample size $n$ and $z=1.96$ (95\% CI), Wilson interval is:
\begin{equation}
\frac{\hat{p} + \frac{z^2}{2n} \pm
z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{z^2}{4n^2}}}
{1+\frac{z^2}{n}}
\label{eq:wilson}
\end{equation}
This is used to report uncertainty on small pilot datasets and avoid over-claiming.

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{m{2.4cm}c m{2.4cm}c m{2.4cm}c m{2.4cm}}
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Freeze\\Dataset}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Run\\Benchmark}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Compute\\95\% CI}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Claim\\Gate}}
\end{tabular}
\caption{Evaluation protocol and claim gate used for pilot reporting.}
\label{fig:gate}
\end{figure}

\section{Experiments and Results}
\subsection{Ablation: ASP-only vs ASP+LLM (2026-02-19)}
We compare two operating modes on the 70 evaluable cases:
\begin{itemize}[nosep]
  \item \textbf{ASP-only}: keyword router $\rightarrow$ ASP rule engine $\rightarrow$ offline heuristic supervisor (no LLM calls).
  \item \textbf{ASP+LLM}: keyword router $\rightarrow$ ASP rule engine $\rightarrow$ LLM adjudication via Qwen-2.5-7B-Instruct (local Ollama).
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Mode & Accuracy & Wilson 95\% CI & Correct / Total \\
\midrule
ASP-only (offline) & 58.6\% & [0.469, 0.694] & 41 / 70 \\
ASP+Ollama (Qwen-2.5-7B) & \textbf{64.3\%} & [0.526, 0.745] & 45 / 70 \\
ASP+DeepSeek (API) & 68.6\% & [0.570, 0.782] & 48 / 70 \\
\bottomrule
\end{tabular}
\caption{Ablation comparison on the 70-case evaluable benchmark.}
\label{tab:ablation}
\end{table}

The LLM layer improves accuracy by 5.7 percentage points over the symbolic-only baseline. Results are reported with \texttt{temperature=0} for reproducibility. An earlier non-deterministic run (\texttt{temperature=1.0}) achieved 70.0\%, illustrating that LLM variance is a significant confound in small-$n$ evaluations.

\paragraph{Rule over-specification.}\label{sec:overspec}
Increasing ASP rule coverage from 71 to 95 rules (100\% coverage) paradoxically decreased ASP+LLM accuracy by 7.1 percentage points. Analysis revealed that additional symbolic facts biased the LLM toward adat-dominant (B) classification, causing conflict cases (gold=C) to be misclassified as B. This suggests that neuro-symbolic integration requires careful calibration of symbolic information exposure rather than maximizing rule coverage.

\begin{table}[h]
\centering
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{ASP-only} & \multicolumn{3}{c}{ASP+LLM} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Label & P & R & Support & P & R & Support \\
\midrule
A & 0.36 & 0.67 & 6 & 0.36 & 0.83 & 6 \\
B & 0.66 & 0.61 & 31 & 0.75 & 0.77 & 31 \\
C & 0.62 & 0.58 & 31 & 0.83 & 0.65 & 31 \\
D & 0.00 & 0.00 & 2 & 0.00 & 0.00 & 2 \\
\bottomrule
\end{tabular}
\caption{Per-label precision (P) and recall (R) for both modes on the 70-case benchmark.}
\label{tab:labelperf}
\end{table}

\subsection{Extended Error Analysis: The 12 Hard Cases}

Beyond the aggregate error patterns, we conduct a deeper analysis of \textbf{12 cases (17.1\%)} where all three system variants---ASP-only, ASP+Ollama, and ASP+DeepSeek---fail simultaneously. These ``hard cases'' reveal systematic limitations in the current rule set and router design.

\subsubsection{Gold label distribution in hard cases}

Table~\ref{tab:hardcases-dist} shows that conflict cases (C) are over-represented in failures, comprising 50.0\% of hard cases versus 44.3\% overall, confirming that detecting implicit national--adat conflicts remains the primary challenge.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Gold Label & Hard Cases & Overall & Ratio \\
\midrule
C (Conflict) & 6 (50.0\%) & 31 (44.3\%) & 1.13$\times$ \\
B (Adat) & 5 (41.7\%) & 31 (44.3\%) & 0.94$\times$ \\
A (National) & 1 (8.3\%) & 6 (8.6\%) & 0.97$\times$ \\
D (Unclear) & 0 (0.0\%) & 2 (2.9\%) & 0.00$\times$ \\
\bottomrule
\end{tabular}
\caption{Label distribution in 12 hard cases versus overall benchmark (N=70).}
\label{tab:hardcases-dist}
\end{table}

\subsubsection{Dominant failure patterns}

Table~\ref{tab:failure-patterns} categorizes the failure modes. The dominant pattern is \textbf{C$\rightarrow$B misclassification} (33.3\% of hard cases): conflicts where ethnic keywords dominate the query text, causing the symbolic router to overwhelm conflict signals.

\begin{table}[h]
\centering
\begin{tabular}{llcc}
\toprule
Pattern & Description & Count & Percentage \\
\midrule
C$\rightarrow$B & Conflict misclassified as Adat & 4 & 33.3\% \\
B$\rightarrow$A & Adat misclassified as National & 3 & 25.0\% \\
B$\rightarrow$C & Adat misclassified as Conflict & 2 & 16.7\% \\
C$\rightarrow$A & Conflict misclassified as National & 2 & 16.7\% \\
A$\rightarrow$C & National misclassified as Conflict & 1 & 8.3\% \\
\bottomrule
\end{tabular}
\caption{Failure patterns in 12 hard cases where all three systems fail.}
\label{tab:failure-patterns}
\end{table}

The prevalence of C$\rightarrow$B errors suggests that when customary-law keywords (e.g., ``Minangkabau,'' ``pusako,'' ``gono-gini'') appear prominently, the keyword router defaults to the pure-adat pathway even when national-law dimensions are implicitly present. This is particularly evident in Minangkabau cases involving joint property (\textit{harta bersama}), where ownership disputes span both customary inheritance norms and national civil code provisions.

\subsubsection{Domain-specific challenges}

Minangkabau cases are over-represented in failures (33.3\% of hard cases), particularly around:
\begin{itemize}[nosep]
  \item \textit{Harta pusako} (inherited property) classification,
  \item Inter-ethnic marriage property regimes,
  \item Joint business property ownership disputes.
\end{itemize}

This reflects the complexity of matrilineal inheritance rules and their intersection with national civil law, which the current ASP rule base captures incompletely.

\subsubsection{D-label complete failure}

All three systems fail on both D (``clarification required'') cases in the benchmark (0/2 recall). The system consistently assigns substantive labels (A, B, or C) even when the scenario contains insufficient information for legal determination. This indicates that \textbf{uncertainty detection} is not supported by the current rule set or prompt design---a critical gap for real-world deployment where refusing to classify is often the correct response.

\subsubsection{Root cause summary}

The fact that all three systems fail identically on these 12 cases indicates the problem lies in the \textbf{shared ASP rule set} or \textbf{router/prompt design}, not in LLM backend choice. The primary failure modes are:
\begin{enumerate}[nosep]
  \item \textbf{Implicit conflict detection}: When conflict keywords are subtle (e.g., ``mendahului,'' ``tidak sah'') and ethnic keywords dominate, the router misses the conflict signal.
  \item \textbf{Domain ambiguity}: ``General'' cases without clear domain markers default to national-law pathways, missing adat dimensions.
  \item \textbf{No abstention mechanism}: The system cannot output D even when confidence is low.
\end{enumerate}

\subsection{Per-Domain Performance Analysis}

Table~\ref{tab:domain} breaks down accuracy by customary law domain. Two patterns are noteworthy.

\begin{table}[h]
\centering
\begin{tabular}{lrrrr}
\toprule
Domain & N & ASP-only & ASP+Ollama & ASP+DeepSeek \\
\midrule
Minangkabau & 21 & 71.4\% & 76.2\% & 71.4\% \\
Bali        & 21 & 71.4\% & 81.0\% & 76.2\% \\
Jawa        & 17 & 35.3\% & 29.4\% & 52.9\% \\
Nasional    &  7 & 42.9\% & 71.4\% & 71.4\% \\
Lintas      &  4 & 50.0\% & 50.0\% & 50.0\% \\
\midrule
Overall     & 70 & 58.6\% & 64.3\% & 68.6\% \\
\bottomrule
\end{tabular}
\caption{Per-domain accuracy across three system configurations.}
\label{tab:domain}
\end{table}

\paragraph{LLM helps Bali and Nasional, hurts Jawa.}
ASP+Ollama improves substantially over ASP-only for Bali (+9.6pp) and Nasional (+28.5pp), but \textbf{decreases} accuracy for Jawa (35.3\% $\rightarrow$ 29.4\%, $-$5.9pp). This is a counter-intuitive negative finding: adding an LLM adjudication layer makes Jawa cases \textit{worse}. Analysis of Jawa errors reveals that bilateral inheritance rules (gono-gini, sigar semangka) are frequently misclassified as national-law (B$\rightarrow$A), suggesting the LLM over-weights civil code terminology present in Jawa case descriptions. DeepSeek partially recovers this deficit (52.9\%), indicating LLM capability is a factor, but the underlying routing ambiguity for Jawa remains unresolved.

\subsection{Cross-Validation Across LLM Backends}

To assess whether results are robust to LLM choice, we evaluate three operating configurations on the identical 70-case benchmark:
\begin{itemize}[nosep]
  \item \textbf{ASP-only}: offline heuristic supervisor (no LLM calls);
  \item \textbf{ASP+Ollama}: Qwen-2.5-7B-Instruct via local Ollama;
  \item \textbf{ASP+DeepSeek}: DeepSeek-Chat via API.
\end{itemize}

\subsubsection{Accuracy comparison}

Table~\ref{tab:crossval} reports accuracy and agreement metrics. ASP+DeepSeek achieves the highest accuracy at 68.6\%, followed by ASP+Ollama at 64.3\%, with ASP-only at 58.6\%.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Mode & Accuracy & Wilson 95\% CI & Cohen's $\kappa$ & Agreement \\
\midrule
ASP-only (offline) & 58.6\% & [0.469, 0.694] & 0.331 & Fair \\
ASP+Ollama (Qwen-2.5-7B) & 64.3\% & [0.526, 0.745] & 0.418 & Moderate \\
ASP+DeepSeek (API) & \textbf{68.6\%} & [0.570, 0.782] & 0.483 & Moderate \\
\bottomrule
\end{tabular}
\caption{Cross-validation across three LLM backends (N=70 evaluable cases). All runs use identical rule state (post-rollback, 71 rules) and \texttt{temperature=0}.}
\label{tab:crossval}
\end{table}

\subsubsection{Statistical significance}

Table~\ref{tab:mcnemar} presents pairwise McNemar tests.
\textbf{None of the differences reach statistical significance} at $\alpha=0.05$: ASP-only vs.~Ollama yields $p = 0.344$, ASP-only vs.~DeepSeek yields $p = 0.167$, and Ollama vs.~DeepSeek yields $p = 0.549$. The largest effect is ASP-only vs.~DeepSeek (19 discordant pairs, DeepSeek favored by +7), limited by small sample size ($n=70$).

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Comparison & $p$-value & Discordant & Favored \\
\midrule
ASP-only vs.~Ollama & 0.344 & 10 & Ollama (+4) \\
ASP-only vs.~DeepSeek & 0.167 & 19 & DeepSeek (+7) \\
Ollama vs.~DeepSeek & 0.549 & 11 & DeepSeek (+3) \\
\bottomrule
\end{tabular}
\caption{McNemar exact binomial test results for pairwise accuracy differences (N=70).}
\label{tab:mcnemar}
\end{table}

\subsubsection{Inter-system agreement}

Despite using different LLM backends, the three systems exhibit \textbf{substantial agreement} with each other. Fleiss' $\kappa = 0.633$ indicates that the ASP+LLM architecture produces consistent predictions regardless of the specific LLM used. Table~\ref{tab:agreement} breaks down cross-model agreement patterns.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Agreement Pattern & Count & Percentage \\
\midrule
All 3 agree (correct) & 34 & 48.6\% \\
All 3 agree (wrong) & 13 & 18.6\% \\
\midrule
\textbf{All 3 agree (total)} & \textbf{47} & \textbf{67.1\%} \\
2 agree, 1 different & 21 & 30.0\% \\
All 3 different & 2 & 2.9\% \\
\bottomrule
\end{tabular}
\caption{Cross-model agreement among ASP-only, Ollama, and DeepSeek (N=70).}
\label{tab:agreement}
\end{table}

The high unanimous agreement (67.1\%) with only 2.9\% complete disagreement suggests that the symbolic rule layer acts as a \textbf{strong prior} that dominates LLM variation. This is a desirable property for reproducibility: the system's behavior is anchored by the expert-verified ASP rules rather than being sensitive to LLM choice.

\subsubsection{Majority vote analysis}

For cases where two systems agree and one differs, majority vote achieves only \textbf{52.4\% accuracy} (11/21). This indicates that when systems disagree, the majority is \textbf{no more likely to be correct than random chance}. Consequently, simple ensemble strategies offer no improvement over single-system predictions in this setting.

\subsubsection{Implications}

The cross-validation results support two key claims:
\begin{enumerate}[nosep]
  \item \textbf{Reproducibility}: A local 7B parameter model (Qwen-2.5-7B) achieves performance competitive with a commercial API (DeepSeek), with only a 4.3 percentage point gap that is not statistically significant. This supports deployment in resource-constrained settings where local inference is preferred.
  \item \textbf{Rule-centric design}: The high inter-system agreement (Fleiss' $\kappa = 0.633$) confirms that the ASP rule layer, not the LLM backend, is the primary determinant of system behavior. This validates our design choice of treating the LLM as an adjudication interface rather than the core reasoning engine.
\end{enumerate}

\section{Threats to Validity and Limitations}
\begin{enumerate}[nosep]
  \item \textbf{Sample size}: $n=70$ evaluable cases provide insufficient statistical power to detect differences between LLM backends; McNemar $p$-values are all $>0.05$.
  
  \item \textbf{Expert agreement}: Only two expert raters with initial Cohen's $\kappa = 0.394$ (fair agreement), though this improved to 94\% after rubric refinement.
  
  \item \textbf{Class imbalance}: Label D has only 2 cases (2.9\%), resulting in 0\% recall across all systems. This extreme imbalance precludes meaningful evaluation of uncertainty detection.
  
  \item \textbf{Rule over-specification paradox}: Adding more ASP rules (71$\rightarrow$95) decreased accuracy by 7.1 percentage points. This reveals a trade-off between rule coverage and LLM usability that requires careful calibration.
  
  \item \textbf{Rule--router alignment bottleneck}: The 12 hard cases where all systems fail suggest that router design, not rule coverage alone, is the primary limitation. Additional rules without improved keyword mapping will not improve performance.
  
  \item \textbf{Single-language evaluation}: All cases are in Indonesian; generalization to other legal pluralism contexts (e.g., multilingual settings, other customary law systems) is not established.
  
  \item \textbf{Static benchmark}: The benchmark is frozen at 74 cases with 4 disputed; expansion to 100+ cases is needed for narrower confidence intervals.

  \item \textbf{Insufficient statistical power}: Based on observed discordant pairs (10 between ASP-only and Ollama), the current design achieves estimated power of $\approx$0.24 for McNemar at $\alpha=0.05$. Achieving power $\geq$0.8 requires approximately 275--344 evaluable cases under similar effect sizes.
\end{enumerate}

\section{Data and Code Availability}
The dataset and source code are publicly available at:
\begin{center}
\url{https://github.com/neimasilk/nusantara-agent.git}
\end{center}

\section{Conclusion and Next Steps}
We demonstrate that expert-verified ASP rules combined with LLM adjudication achieve 64.3\% accuracy on a 70-case plural-law benchmark, a 5.7-point improvement over ASP-only keyword heuristics.
The result confirms that symbolic customary-law encoding provides measurable value as a reasoning anchor for LLM-based legal classification in Indonesia.
Immediate next steps are:
\begin{enumerate}[nosep]
  \item evaluate additional LLM backends (Kimi, GPT-4o) for broader robustness assessment,
  \item redesign router/prompt to better handle implicit conflict detection and abstention (D-label),
  \item expand the benchmark to 100+ cases and resolve the 4 remaining disputed cases,
  \item investigate optimal symbolic information exposure levels for neuro-symbolic calibration.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{paper/references}

\end{document}
