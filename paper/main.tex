\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{array}

\title{Neuro-Symbolic Legal Reasoning with Expert-Verified Customary Law Rules: A Pilot Study on Indonesian Legal Pluralism}
\author{
Anonymous Authors\\
Project: Nusantara-Agent
}
\date{Draft v0.6 -- 2026-02-24}

\begin{document}
\maketitle

\begin{abstract}
We present \textit{Nusantara-Agent}, a neuro-symbolic framework for legal reasoning in Indonesian legal pluralism, where national and customary (\textit{adat}) norms can conflict.
Our primary contribution is resource-oriented: an expert-verified customary-law rule base encoded in Answer Set Programming (ASP) across three domains (Minangkabau 25 rules, Bali 34 rules, Jawa 36 rules), together with an auditable pilot benchmark protocol.
An LLM adjudication layer is used as an interface over symbolic outputs.
On 74 dual-expert-labeled scenarios (70 evaluable, 4 disputed), we compare ASP-only (58.6\%, Wilson 95\% CI [0.469, 0.694]), ASP+Ollama/deepseek-r1 (64.3\%, CI [0.526, 0.745]), and ASP+DeepSeek (68.6\%, CI [0.570, 0.782]).
Observed gains over ASP-only (+5.7 pp and +10.0 pp) are directionally positive but statistically inconclusive at pilot scale: all pairwise McNemar tests are non-significant ($p \geq 0.17$, $n=70$).
Inter-system agreement remains substantial (Fleiss' $\kappa = 0.638$), indicating stable rule-layer behavior across backends.
We therefore position this work as a pilot benchmark and methodology paper, not a definitive efficacy claim; reaching statistical power $\geq$0.8 is estimated to require approximately 344 evaluable cases.
\end{abstract}

\section{Introduction}

The Indonesian legal landscape is characterized by deep legal pluralism, where codified national law coexists with diverse systems of customary law (\textit{adat}). Reasoning in this setting is not only a retrieval problem; it requires reconciling non-monotonic conflicts between overlapping legal regimes. For AI systems, this is difficult because outcomes often depend on cross-regime conflict handling that is implicit, exception-heavy, and procedurally constrained.

\paragraph{Research Gap.}
Existing legal NLP research focuses predominantly on monolithic, codified systems, mostly in Western jurisdictions. Typical tasks (judgment prediction, contract extraction, summarization) assume one dominant legal source. In the Indonesian context, legal document classification exists, but executable reasoning frameworks for customary law remain rare. In particular, there is no established benchmark line that combines expert-verified Indonesian \textit{adat} rules, formal ASP encoding, and auditable pilot evaluation for plural-law classification.

\paragraph{Contributions.}
This paper targets that gap with a resource-first neuro-symbolic contribution. We use ASP as the rule-centric reasoning core and treat the LLM layer as an adjudication interface. Our concrete contributions are:
\begin{itemize}[nosep]
  \item A formally encoded, expert-verified ASP rule base covering three Indonesian customary law domains: Minangkabau (25 rules), Bali (34 rules), and Jawa (36 rules). Of these 95 rules, 71 are currently active in the reasoning engine.
  \item A pilot benchmark resource and reporting protocol over 74 dual-expert-labeled scenarios (70 evaluable agreed cases, 4 disputed), including manifest-based provenance and explicit claim-gating.
  \item Cross-backend empirical results showing directional but non-conclusive gains and substantial inter-system agreement (Fleiss' $\kappa = 0.638$), supporting rule-layer stability as an anchor hypothesis.
  \item Transparent documentation of negative findings, including the ``Rule Over-specification Paradox'', backend sensitivity, complete D-label failure (0\% recall), and pilot-scale statistical limits.
\end{itemize}

\paragraph{Scope and Limitations.}
Following the strategic pivot on 2026-02-12, this manuscript focuses on rule formalization, expert verification workflow, and pilot-scale evaluation. The current results are explicitly preliminary. First, statistical power is limited ($n=70$ evaluable; all pairwise McNemar comparisons are non-significant, $p \geq 0.17$). Second, the same 70 evaluable cases have been used during development and final reporting, so a fully clean held-out test set is not yet available in this pilot cycle. Accordingly, we frame the paper as a benchmark-and-methodology contribution for legal pluralism, while deferring strong efficacy and generalization claims to future unseen-case evaluation.

\paragraph{Paper Roadmap.}
The remainder of this paper is organized as follows: Section 2 reviews related work in neuro-symbolic AI and legal NLP; Section 3 defines the multi-label classification task; Section 4 provides a system overview of the \textit{Nusantara-Agent} architecture; Section 5 details the expert annotation protocol and dataset construction; Section 6 defines the evaluation metrics; Section 7 presents our experimental results and error analysis; Section 8 discusses limitations of the pilot study; Section 9 provides data and code availability; and Section 10 concludes with future directions.

\section{Related Work}
This work is situated at the intersection of neuro-symbolic AI, legal-domain NLP, and the formalization of Indonesian legal pluralism.

\paragraph{Neuro-symbolic AI and Symbolic Reasoning.}
The neuro-symbolic paradigm, which integrates neural learning with symbolic knowledge representation, has been identified as the ``3rd wave'' of AI \cite{garcez2023neurosymbolic}. While neural models excel at pattern recognition in unstructured text, they often struggle with the strict logical consistency and auditability required for legal adjudication. Answer Set Programming (ASP) provides a powerful declarative formalism for non-monotonic reasoning \cite{lifschitz2019asp}, with Clingo serving as a state-of-the-art solver \cite{gebser2014clingo}. ASP is uniquely suited for legal domains characterized by defaults, exceptions, and conflicting norms; however, purely symbolic systems face significant hurdles in mapping natural language legal scenarios to formal predicates without a neural processing layer.

\paragraph{LLMs and Retrieval-Augmented Reasoning.}
Foundational work in Large Language Models (LLMs) and prompting techniques—including Transformers, few-shot scaling, and chain-of-thought reasoning—has demonstrated impressive capabilities in text-based reasoning \cite{vaswani2017attention,brown2020language,wei2022cot,wang2022selfconsistency,yao2023react,yao2023tree,shinn2023reflexion}. To ground these models in factual data, researchers have developed various retrieval-augmented generation (RAG) and hierarchical retrieval strategies \cite{lewis2020rag,asai2023selfrag,gao2023ragsurvey,sarthi2024raptor,he2024gretriever}. Despite these advancements, LLMs remain susceptible to reasoning hallucinations when navigating the intricate, non-monolithic hierarchies of plural legal systems, necessitating a more robust symbolic anchoring mechanism.

\paragraph{Legal NLP and Indonesian Legal Pluralism.}
Existing legal NLP research focuses primarily on benchmarks for judgment prediction, contract extraction, and legal summarization within monolithic Western jurisdictions \cite{guha2023legalbench,colombo2024saullm,sie-etal-2024-summarizing,liu-etal-2024-enhancing-legal,nigam-etal-2024-rethinking,xie-etal-2024-clc,mali-etal-2024-information,narendra-etal-2024-enhancing,hou-etal-2024-gaps,kwak-etal-2024-classify,taranukhin-etal-2024-empowering,tran-etal-2024-deberta,tyss-etal-2024-lexsumm,tyss-etal-2024-supporting}. These tasks typically assume a single source of legal authority. In contrast, Indonesian legal pluralism—the coexistence of national law and diverse customary (\textit{adat}) systems—has been extensively studied in legal anthropology \cite{hooker1978adat,burns2004leiden}. While these studies provide rich qualitative insights into customary norms, they do not offer the formal, executable computational representations required for automated reasoning.

\paragraph{Research Gap.}
A critical research gap exists: no prior work has computationally formalized Indonesian customary law to enable auditable, neuro-symbolic reasoning. Nusantara-Agent addresses this by encoding expert-verified \textit{adat} rules in ASP to provide a symbolic anchor for LLM-based adjudication, thereby allowing consistent behavior in pluralistic legal environments.

\section{Task Definition}
Given a legal scenario $x$, the system predicts one of four policy labels:
\begin{itemize}[nosep]
  \item \textbf{A}: mainly national-law resolution,
  \item \textbf{B}: mainly customary-law resolution,
  \item \textbf{C}: synthesis of national + customary law,
  \item \textbf{D}: clarification required.
\end{itemize}

\noindent
Predictions are compared against expert labels with explicit agreement status.

\section{System Overview}
Nusantara-Agent is rule-centric: ASP formalization of expert-verified customary law is the primary modeling contribution.
The LLM-based adjudication layer is used as a practical decision interface and is not the central novelty claim.

This design choice is motivated by three requirements specific to legal reasoning in pluralistic jurisdictions: (1) auditability---ASP rules provide explicit, inspectable reasoning traces that satisfy legal domain expectations for transparency; (2) non-monotonic reasoning---ASP's native support for defaults and exceptions accommodates the defeasible nature of customary law norms; and (3) deployment flexibility---the system can operate without API calls in offline mode using deterministic heuristics. Pilot evidence is consistent with this architecture: despite using different LLM backends (local 7B-parameter model versus commercial API), the three system configurations exhibit substantial inter-system agreement (Fleiss' $\kappa = 0.638$), suggesting that the ASP rule layer contributes strongly to shared behavior across backends.

\subsection{Core Neuro-Symbolic Components}
\begin{itemize}[nosep]
  \item \textbf{Keyword router and fact extraction} for domain routing and ASP fact grounding. This component maps natural language case descriptions to formal predicates, enabling the symbolic engine to reason over unstructured input without requiring an explicit knowledge graph.
  \item \textbf{ASP rule engine (Clingo)} for hard legal constraints and contradiction signals. Unlike pure retrieval-augmented systems, this engine performs logical inference to detect regime conflicts and apply non-monotonic defaults, distinguishing Nusantara-Agent from standard RAG pipelines.
  \item \textbf{Optional LLM adjudication} for final label synthesis; offline fallback for deterministic operation. The LLM layer serves as a trainable decision interface that can be removed entirely without breaking the pipeline, providing resilience against API unavailability or cost constraints.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lrrl}
\toprule
Domain & Expert-Verified Rules & ASP Rule File & Scope \\
\midrule
Minangkabau & 25 & \texttt{minangkabau.lp} & Matrilineal inheritance and pusako norms \\
Bali & 34 & \texttt{bali.lp} & Purusa/sentana and druwe classifications \\
Jawa & 36 & \texttt{jawa.lp} & Bilateral inheritance and gono-gini patterns \\
\midrule
Total & 95 & 3 domains & Expert-verified customary law base \\
\bottomrule
\end{tabular}
\caption{Expert-verified customary-law rules. Of 95 total, 71 are currently encoded in ASP; the remaining 24 were found to cause accuracy regression when added (see Section~\ref{sec:overspec}).}
\label{tab:rulebase}
\end{table}

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{m{2.6cm}c m{2.6cm}c m{2.6cm}c m{2.6cm}}
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Input\\Case Query}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Router +\\Fact Extraction}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering ASP Rule\\Engine}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Label Proposal\\(A/B/C/D)}}
\end{tabular}

\vspace{0.4em}
\begin{tabular}{m{3.6cm}c m{3.6cm}c m{3.6cm}}
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Optional LLM\\Adjudication}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Offline Safety Net\\(No API Mode)}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Final Report\\+ Trace}}
\end{tabular}
\caption{High-level architecture of Nusantara-Agent.}
\label{fig:architecture}
\end{figure}

\section{Dataset and Expert Protocol}
\subsection{Current Snapshot (2026-02-19)}
The benchmark dataset was constructed in two phases: an initial batch of 24 cases and an expanded batch of 50 new cases.
Each case was independently labeled by two qualified legal experts (Ahli-1: Dr.~Hendra Kusuma, S.H., M.Hum.; Ahli-2: Dr.~Indra Gunawan, S.H., M.H.).

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Item & Value \\
\midrule
Total cases in benchmark & 74 \\
Evaluable agreed cases & 70 \\
Disputed cases (excluded from accuracy) & 4 \\
Initial batch agreement (24 cases) & 58.3\% (Cohen's $\kappa$ = 0.394) \\
Expanded batch agreement (50 cases) & 94.0\% (47/50) \\
Benchmark manifest source & \texttt{data/benchmark\_manifest.json} \\
\bottomrule
\end{tabular}
\caption{Operational dataset status.}
\label{tab:status}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
Gold Label & Count & Percentage \\
\midrule
A & 6 & 8.6\% \\
B & 31 & 44.3\% \\
C & 31 & 44.3\% \\
D & 2 & 2.9\% \\
\bottomrule
\end{tabular}
\caption{Label distribution in the evaluable benchmark subset (N=70).}
\label{tab:labeldist}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Agreement Metric & Value \\
\midrule
Initial batch agreed/total & 14/24 (58.3\%) \\
Expanded batch agreed/total & 47/50 (94.0\%) \\
Adjudicated (initial disputed) & 9/10 resolved \\
Remaining disputed & 4/74 (5.4\%) \\
Accuracy reporting policy & Computed only on agreed subset \\
\bottomrule
\end{tabular}
\caption{Agreement profile across annotation phases.}
\label{tab:consensus}
\end{table}

\subsection{Rubric refinement}
Initial inter-rater agreement in the 24-case batch was moderate ($\kappa$ = 0.394; 14/24 agreement), with recurring ambiguity at the A/B/C decision boundary when adat context and national-law triggers co-occurred.
The annotation protocol was then tightened in stages: (i) locked A/B/C/D label definitions, (ii) explicit boundary tests for A vs.~C, and (iii) a mandatory decision checklist (dominance, dual-requirement, missing-fact check) to justify why synthesis is or is not required.
The expanded 50-case packet applied this refined rubric on a disjoint GS case pool and explicitly instructed raters not to inspect prior labels.
In the current dataset snapshot, Ahli-1 vs.~Ahli-2 agreement on the expanded GS pool is 47/50 (94.0\%), with only three remaining disagreements (GS-0022, GS-0023, GS-0032), all at the B/C boundary and currently marked as disputed.
A complete audit trail of this refinement process is maintained in the project methodology log (\texttt{docs/methodology/rubric\_refinement\_log.md}).
The same log also records project-level governance sign-off for the rubric protocol.

\subsection{Adjudication policy}
For the current evaluation:
\begin{itemize}[nosep]
  \item only agreed cases are used for quantitative accuracy reporting,
  \item disputed cases are excluded and queued for third-expert tiebreak,
  \item benchmark manifest is treated as the reporting authority for snapshot counts.
\end{itemize}

\section{Metrics and Equations}
To avoid ambiguous reporting, we explicitly define pilot metrics.

\subsection{Agreement Coverage}
Let $N$ be total active cases and $N_a$ the agreed/evaluable subset.
\begin{equation}
\mathrm{ConsensusStrength} = \frac{N_a}{N}
\label{eq:consensus_strength}
\end{equation}
For the current snapshot, $(N_a, N)=(70,74)$, so:
\begin{equation}
\mathrm{ConsensusStrength} = \frac{70}{74}=0.946
\end{equation}

\subsection{Dispute Rate}
Let $N_d$ be the number of disputed cases:
\begin{equation}
\mathrm{TieRate} = \frac{N_d}{N}
\label{eq:tierate}
\end{equation}
With $N_d=4$ and $N=74$, $\mathrm{TieRate}=0.054$.

\subsection{Binomial Confidence Interval (Wilson)}
For observed accuracy $\hat{p}$ with sample size $n$ and $z=1.96$ (95\% CI), Wilson interval is:
\begin{equation}
\frac{\hat{p} + \frac{z^2}{2n} \pm
z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{z^2}{4n^2}}}
{1+\frac{z^2}{n}}
\label{eq:wilson}
\end{equation}
This is used to report uncertainty on small pilot datasets and avoid over-claiming.

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{m{2.4cm}c m{2.4cm}c m{2.4cm}c m{2.4cm}}
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Freeze\\Dataset}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Run\\Benchmark}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Compute\\95\% CI}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Claim\\Gate}}
\end{tabular}
\caption{Evaluation protocol and claim gate used for pilot reporting.}
\label{fig:gate}
\end{figure}

\section{Experiments and Results}
\subsection{Ablation: ASP-only vs ASP+LLM (2026-02-19)}
We compare two operating modes on the 70 evaluable cases:
\begin{itemize}[nosep]
  \item \textbf{ASP-only}: keyword router $\rightarrow$ ASP rule engine $\rightarrow$ offline heuristic supervisor (no LLM calls).
  \item \textbf{ASP+LLM}: keyword router $\rightarrow$ ASP rule engine $\rightarrow$ LLM adjudication via deepseek-r1 (local Ollama).
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Mode & Accuracy & Wilson 95\% CI & Correct / Total \\
\midrule
ASP-only (offline) & 58.6\% & [0.469, 0.694] & 41 / 70 \\
ASP+Ollama (deepseek-r1) & \textbf{64.3\%} & [0.526, 0.745] & 45 / 70 \\
ASP+DeepSeek (API) & 68.6\% & [0.570, 0.782] & 48 / 70 \\
\bottomrule
\end{tabular}
\caption{Ablation comparison on the 70-case evaluable benchmark.}
\label{tab:ablation}
\end{table}

The LLM layer shows an observed improvement of 5.7 pp (Ollama) and 10.0 pp (DeepSeek) over the symbolic-only baseline. However, as reported in Section~\ref{sec:crossval}, all pairwise McNemar tests are non-significant ($p \geq 0.17$, $n=70$); these observed differences should be interpreted as pilot observations pending larger-scale evaluation. Results are reported with \texttt{temperature=0} for reproducibility. An earlier non-deterministic run (\texttt{temperature=1.0}) achieved 70.0\%, illustrating that LLM variance is a significant confound in small-$n$ evaluations.

\paragraph{Rule over-specification.}\label{sec:overspec}
Increasing ASP rule coverage from 71 to 95 rules (100\% coverage) paradoxically decreased ASP+LLM accuracy by 7.1 percentage points. Analysis revealed that additional symbolic facts biased the LLM toward adat-dominant (B) classification, causing conflict cases (gold=C) to be misclassified as B. This suggests that neuro-symbolic integration requires careful calibration of symbolic information exposure rather than maximizing rule coverage.

\begin{table}[h]
\centering
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{ASP-only} & \multicolumn{3}{c}{ASP+LLM} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Label & P & R & Support & P & R & Support \\
\midrule
A & 0.36 & 0.67 & 6 & 0.36 & 0.83 & 6 \\
B & 0.66 & 0.61 & 31 & 0.75 & 0.77 & 31 \\
C & 0.62 & 0.58 & 31 & 0.83 & 0.65 & 31 \\
D & 0.00 & 0.00 & 2 & 0.00 & 0.00 & 2 \\
\bottomrule
\end{tabular}
\caption{Per-label precision (P) and recall (R) for both modes on the 70-case benchmark.}
\label{tab:labelperf}
\end{table}

\subsection{Extended Error Analysis: The 12 Hard Cases}

Beyond the aggregate error patterns, we conduct a deeper analysis of \textbf{12 cases (17.1\%)} where all three system variants---ASP-only, ASP+Ollama, and ASP+DeepSeek---fail simultaneously. These ``hard cases'' reveal systematic limitations in the current rule set and router design.

\subsubsection{Gold label distribution in hard cases}

Table~\ref{tab:hardcases-dist} shows that conflict cases (C) are over-represented in failures, comprising 50.0\% of hard cases versus 44.3\% overall. This pattern suggests that detecting implicit national--adat conflicts is likely one of the primary challenges in the current benchmark.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Gold Label & Hard Cases & Overall & Ratio \\
\midrule
C (Conflict) & 6 (50.0\%) & 31 (44.3\%) & 1.13$\times$ \\
B (Adat) & 5 (41.7\%) & 31 (44.3\%) & 0.94$\times$ \\
A (National) & 1 (8.3\%) & 6 (8.6\%) & 0.97$\times$ \\
D (Unclear) & 0 (0.0\%) & 2 (2.9\%) & 0.00$\times$ \\
\bottomrule
\end{tabular}
\caption{Label distribution in 12 hard cases versus overall benchmark (N=70).}
\label{tab:hardcases-dist}
\end{table}

\subsubsection{Dominant failure patterns}

Table~\ref{tab:failure-patterns} categorizes the failure modes. The dominant pattern is \textbf{C$\rightarrow$B misclassification} (33.3\% of hard cases): conflicts where ethnic keywords dominate the query text, causing the symbolic router to overwhelm conflict signals.

\begin{table}[h]
\centering
\begin{tabular}{llcc}
\toprule
Pattern & Description & Count & Percentage \\
\midrule
C$\rightarrow$B & Conflict misclassified as Adat & 4 & 33.3\% \\
B$\rightarrow$A & Adat misclassified as National & 3 & 25.0\% \\
B$\rightarrow$C & Adat misclassified as Conflict & 2 & 16.7\% \\
C$\rightarrow$A & Conflict misclassified as National & 2 & 16.7\% \\
A$\rightarrow$C & National misclassified as Conflict & 1 & 8.3\% \\
\bottomrule
\end{tabular}
\caption{Failure patterns in 12 hard cases where all three systems fail.}
\label{tab:failure-patterns}
\end{table}

The prevalence of C$\rightarrow$B errors suggests that when customary-law keywords (e.g., ``Minangkabau,'' ``pusako,'' ``gono-gini'') appear prominently, the keyword router defaults to the pure-adat pathway even when national-law dimensions are implicitly present. This is particularly evident in Minangkabau cases involving joint property (\textit{harta bersama}), where ownership disputes span both customary inheritance norms and national civil code provisions.

\subsubsection{Domain-specific challenges}

Minangkabau cases are over-represented in failures (33.3\% of hard cases), particularly around:
\begin{itemize}[nosep]
  \item \textit{Harta pusako} (inherited property) classification,
  \item Inter-ethnic marriage property regimes,
  \item Joint business property ownership disputes.
\end{itemize}

This reflects the complexity of matrilineal inheritance rules and their intersection with national civil law, which the current ASP rule base captures incompletely.

\subsubsection{D-label complete failure}

All three systems fail on both D (``clarification required'') cases in the benchmark (0/2 recall). The system consistently assigns substantive labels (A, B, or C) even when the scenario contains insufficient information for legal determination. This indicates that \textbf{uncertainty detection} is not supported by the current rule set or prompt design---a critical gap for real-world deployment where refusing to classify is often the correct response.

\subsubsection{Root cause summary}

The fact that all three systems fail identically on these 12 cases suggests that the dominant error sources are in the \textbf{shared ASP rule set} and \textbf{router/prompt design}, rather than in backend-specific differences. The primary failure modes are:
\begin{enumerate}[nosep]
  \item \textbf{Implicit conflict detection}: When conflict keywords are subtle (e.g., ``mendahului,'' ``tidak sah'') and ethnic keywords dominate, the router misses the conflict signal.
  \item \textbf{Domain ambiguity}: ``General'' cases without clear domain markers default to national-law pathways, missing adat dimensions.
  \item \textbf{No abstention mechanism}: The system cannot output D even when confidence is low.
\end{enumerate}

\paragraph{Two-layer error diagnosis.}
A finer-grained audit of C$\rightarrow$B errors shows two distinct failure layers. Across 29 total C$\rightarrow$B events (ASP-only=10, ASP+Ollama=10, ASP+DeepSeek=9), 23/29 (79.3\%) occur when no ASP conflict signal is produced, indicating an upstream router/fact-extraction failure rather than an LLM reasoning failure. The remaining 6/29 events (20.7\%) occur despite an ASP conflict signal, meaning the adjudicator still collapses to label B; this pattern is concentrated in DeepSeek (5/6 events). Therefore, mitigation should be split into two tracks: improve router keyword-to-fact mapping for conflict recall, and recalibrate adjudicator prompts for conflict-preserving decisions, both without adding ASP rules (to avoid repeating the F-018 regression).

\subsection{Per-Domain Performance Analysis}

Table~\ref{tab:domain} breaks down accuracy by customary law domain. Two patterns are noteworthy.

\begin{table}[h]
\centering
\begin{tabular}{lrrrr}
\toprule
Domain & N & ASP-only & ASP+Ollama & ASP+DeepSeek \\
\midrule
Minangkabau & 21 & 71.4\% & 76.2\% & 71.4\% \\
Bali        & 21 & 71.4\% & 81.0\% & 76.2\% \\
Jawa        & 17 & 35.3\% & 29.4\% & 52.9\% \\
Nasional    &  7 & 42.9\% & 71.4\% & 71.4\% \\
Lintas      &  4 & 50.0\% & 50.0\% & 50.0\% \\
\midrule
Overall     & 70 & 58.6\% & 64.3\% & 68.6\% \\
\bottomrule
\end{tabular}
\caption{Per-domain accuracy across three system configurations.}
\label{tab:domain}
\end{table}

\paragraph{LLM helps Bali and Nasional, hurts Jawa.}
ASP+Ollama shows larger observed gains over ASP-only for Bali (+9.6pp) and Nasional (+28.5pp) (domain-level McNemar not computed due to small per-domain $n$), but \textbf{decreases} accuracy for Jawa (35.3\% $\rightarrow$ 29.4\%, $-$5.9pp). This is a counter-intuitive negative finding: adding an LLM adjudication layer makes Jawa cases \textit{worse}. Analysis of Jawa errors reveals that bilateral inheritance rules (gono-gini, sigar semangka) are frequently misclassified as national-law (B$\rightarrow$A), suggesting the LLM over-weights civil code terminology present in Jawa case descriptions. DeepSeek partially recovers this deficit (52.9\%), indicating LLM capability is a factor, but the underlying routing ambiguity for Jawa remains unresolved.

\subsection{Cross-Validation Across LLM Backends}
\label{sec:crossval}

To assess whether results are robust to LLM choice, we evaluate three operating configurations on the identical 70-case benchmark:
\begin{itemize}[nosep]
  \item \textbf{ASP-only}: offline heuristic supervisor (no LLM calls);
  \item \textbf{ASP+Ollama}: deepseek-r1 via local Ollama;
  \item \textbf{ASP+DeepSeek}: DeepSeek-Chat via API.
\end{itemize}

\subsubsection{Accuracy comparison}

Table~\ref{tab:crossval} reports accuracy and agreement metrics. ASP+DeepSeek achieves the highest accuracy at 68.6\%, followed by ASP+Ollama at 64.3\%, with ASP-only at 58.6\%.
To maintain methodological comparability, Table~\ref{tab:crossval} reports only the canonical three-configuration snapshot (2026-02-20); additional exploratory backends from 2026-02-23 are discussed as sensitivity checks in Section~\ref{sec:limitations}.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Mode & Accuracy & Wilson 95\% CI & Cohen's $\kappa$ & Agreement \\
\midrule
ASP-only (offline) & 58.6\% & [0.469, 0.694] & 0.331 & Fair \\
ASP+Ollama (deepseek-r1) & 64.3\% & [0.526, 0.745] & 0.418 & Moderate \\
ASP+DeepSeek (API) & \textbf{68.6\%} & [0.570, 0.782] & 0.483 & Moderate \\
\bottomrule
\end{tabular}
\caption{Cross-validation across three LLM backends (N=70 evaluable cases). All runs use identical rule state (post-rollback, 71 rules) and \texttt{temperature=0}.}
\label{tab:crossval}
\end{table}

\subsubsection{Statistical significance}

Table~\ref{tab:mcnemar} presents pairwise McNemar tests.
\textbf{None of the differences reach statistical significance} at $\alpha=0.05$: ASP-only vs.~Ollama yields $p = 0.344$, ASP-only vs.~DeepSeek yields $p = 0.167$, and Ollama vs.~DeepSeek yields $p = 0.549$. The largest effect is ASP-only vs.~DeepSeek (19 discordant pairs, DeepSeek favored by +7), limited by small sample size ($n=70$).

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Comparison & $p$-value & Discordant & Favored \\
\midrule
ASP-only vs.~Ollama & 0.344 & 10 & Ollama (+4) \\
ASP-only vs.~DeepSeek & 0.167 & 19 & DeepSeek (+7) \\
Ollama vs.~DeepSeek & 0.549 & 11 & DeepSeek (+3) \\
\bottomrule
\end{tabular}
\caption{McNemar exact binomial test results for pairwise accuracy differences (N=70).}
\label{tab:mcnemar}
\end{table}

\subsubsection{Inter-system agreement}

Despite using different LLM backends, the three systems exhibit \textbf{substantial agreement} with each other. Fleiss' $\kappa = 0.638$ indicates that the ASP+LLM architecture produces consistent predictions regardless of the specific LLM used. Table~\ref{tab:agreement} breaks down cross-model agreement patterns.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Agreement Pattern & Count & Percentage \\
\midrule
All 3 agree (correct) & 34 & 48.6\% \\
All 3 agree (wrong) & 13 & 18.6\% \\
\midrule
\textbf{All 3 agree (total)} & \textbf{47} & \textbf{67.1\%} \\
2 agree, 1 different & 21 & 30.0\% \\
All 3 different & 2 & 2.9\% \\
\bottomrule
\end{tabular}
\caption{Cross-model agreement among ASP-only, Ollama, and DeepSeek (N=70).}
\label{tab:agreement}
\end{table}

The high unanimous agreement (67.1\%) with only 2.9\% complete disagreement is consistent with the symbolic rule layer acting as a \textbf{strong prior} across backends. In pilot terms, this is a reproducibility-positive signal: system behavior appears more anchored by expert-verified ASP rules than by backend-specific variation.

\subsubsection{Majority vote analysis}

For cases where two systems agree and one differs, majority vote achieves only \textbf{52.4\% accuracy} (11/21). This indicates that when systems disagree, the majority is \textbf{no more likely to be correct than random chance}. Consequently, simple ensemble strategies offer no improvement over single-system predictions in this setting.

\subsubsection{Implications}

The cross-validation results provide pilot-scale evidence for the following observations:
\begin{enumerate}[nosep]
  \item \textbf{Reproducibility feasibility}: In this pilot, a local Ollama backend (deepseek-r1) reaches performance close to a commercial API (DeepSeek), with a 4.3 percentage point gap that remains non-significant. This is feasibility evidence for resource-constrained deployment, not a definitive parity claim.
  \item \textbf{Rule-centric design hypothesis}: The substantial inter-system agreement (Fleiss' $\kappa = 0.638$) is consistent with the ASP rule layer acting as a dominant anchor across backends. At current scale, this should be interpreted as directional support for a rule-centric design, not as a causal proof.
  \item \textbf{Exploratory backend sensitivity}: Two additional local Ollama backends run after the canonical freeze show divergent behavior under the same rule state and prompt. ASP+gpt-oss:20b reaches 45/70 (64.29\%), matching the canonical local Ollama/deepseek-r1 result (45/70), while ASP+Qwen3-14B drops to 38/70 (54.29\%). Against their same-day ASP-only paired baseline (42/70), these correspond to +4.29 pp (McNemar $p=0.646$) and $-5.71$ pp ($p=0.480$), respectively, indicating that neuro-symbolic gains remain backend-dependent.
  \item \textbf{Open-source B$\rightarrow$A bias}: gpt-oss:20b and Qwen3-14B share 6 identical B$\rightarrow$A failures (GS-0010, GS-0014, GS-0019, GS-0020, GS-0021, GS-0031), where strong national-law signals in ASP traces are over-interpreted as national-law dominance despite gold label B. This repeated pattern was not observed with the same concentration in the DeepSeek API run, suggesting backend calibration differences in legal conflict handling.
\end{enumerate}

\section{Limitations}
\label{sec:limitations}

Despite the observed performance improvements in the neuro-symbolic configurations, several critical limitations must be acknowledged to frame the validity of this pilot study correctly.

\paragraph{Statistical Power and Sample Size.}
The primary limitation of this study is the small sample size ($n=70$ evaluable cases). While the ASP+DeepSeek configuration showed a $+10.0$ percentage point (pp) improvement over the ASP-only baseline, this difference is statistically inconclusive at the current scale. All pairwise McNemar tests yielded non-significant results ($p \geq 0.17$), reflecting a lack of statistical power rather than a confirmed absence of effect. Based on the observed number of discordant pairs, we estimate the current study's statistical power to be approximately $0.3$ for a 10pp difference. To achieve a more robust power of $\geq 0.8$ at the observed effect sizes, we estimate that approximately $344$ evaluable cases would be required. Consequently, the findings presented here should be treated as preliminary pilot observations.

\paragraph{Development Contamination and Overfitting.}
The 70 evaluable cases were utilized for both prompt tuning and final evaluation. During the development phase, iterative refinement of the LLM adjudication layer saw accuracy rise from $54.0\%$ to $68.6\%$. This introduces a risk of overfitting to the specific scenarios in the pilot benchmark. At the time of writing, a completely held-out test set has not yet been established. Future work must validate these results on a larger, unseen corpus to ensure that the neuro-symbolic integration generalizes beyond the development set.

\paragraph{Inter-rater Agreement and Rubric Refinement.}
The jump in inter-rater agreement from $58.3\%$ in the first batch ($24$ cases, Cohen's $\kappa = 0.394$) to $94.0\%$ in the second batch ($50$ cases) requires methodological transparency. This improvement was achieved through a mid-study rubric refinement process designed to resolve ambiguities at the boundary between customary law (B) and national--customary conflict (C). While this ensured a high-quality gold standard for the pilot, it also indicates that the classification task is highly sensitive to the definitions provided to the experts. The high agreement in the second batch may partially reflect the raters' alignment with a more prescriptive rubric rather than an inherent clarity in the legal scenarios themselves.

\paragraph{Rule Over-specification and Parity Gaps.}
Our experiments revealed a ``Rule Over-specification Paradox'' (Failure F-018): increasing the ASP rule coverage from 71 to 95 rules paradoxically decreased system accuracy by $7.1$ pp. The additional rules were found to be too general, causing the ASP engine to over-report customary law dominance in conflict cases. This suggests that for neuro-symbolic systems, maximizing rule coverage is secondary to ensuring rule precision and proper calibration between symbolic and neural layers. Furthermore, parity audits (F-015, F-016, F-017) indicate that the current ASP rule base focuses primarily on ``hard'' inheritance constraints and lacks coverage for procedural and contemporary customary norms, particularly in the Jawa domain.

\paragraph{Backend Dependence in Local Deployment.}
Additional post-freeze backend runs indicate that local performance is sensitive to model calibration even under the same neuro-symbolic interface. While canonical ASP+Ollama (deepseek-r1) and exploratory ASP+gpt-oss:20b both reached 45/70 (64.29\%), ASP+Qwen3-14B dropped to 38/70 (54.29\%). All paired McNemar comparisons against their same-day ASP-only baseline remained non-significant (gpt-oss: $p=0.646$; Qwen3: $p=0.480$), so these results should be interpreted as directional evidence rather than confirmed effects. The repeated B$\rightarrow$A overlap across open-source backends (6 shared cases: GS-0010, GS-0014, GS-0019, GS-0020, GS-0021, GS-0031) suggests a structural prompt-calibration issue when national-law cues co-occur with adat-governed outcomes.

\paragraph{Failure in Uncertainty Detection and Domain-Specific Weaknesses.}
The system exhibited a complete failure in uncertainty detection (D-label), with $0\%$ recall across all configurations. The extreme class imbalance (only 2 D-label cases) and the lack of an explicit ``abstention'' mechanism in the prompt design prevented the models from identifying scenarios where clarification was required. Additionally, the Jawa domain remains a significant weakness, with an ASP-only accuracy of only $35.3\%$. The LLM layer frequently misclassified Jawa customary rules as national law (B$\rightarrow$A errors), suggesting that the models over-weight Civil Code terminology present in the case descriptions. These domain-specific disparities indicate that the framework's effectiveness is currently inconsistent across different Indonesian customary systems.

\paragraph{Scope and Generalizability.}
All benchmark cases are in Indonesian, and the ASP rule base encodes customary law from three specific regional systems (Minangkabau, Bali, Jawa). Generalization to other legal pluralism contexts---including other Indonesian customary systems, multilingual settings, or non-Indonesian jurisdictions---is not established by this pilot study.

\section{Data and Code Availability}
The dataset and source code are publicly available at:
\begin{center}
\url{https://github.com/neimasilk/nusantara-agent.git}
\end{center}

\section{Conclusion and Next Steps}
This paper contributes a pilot benchmark-and-methodology resource for AI reasoning in Indonesian legal pluralism: an expert-verified customary-law rule base encoded in ASP, an auditable neuro-symbolic pipeline, and transparent reporting of both positive and negative findings.
On the current 70-case evaluable benchmark, ASP+LLM configurations show observed accuracies of 64.3\%--68.6\% versus 58.6\% for ASP-only (directional gain: +5.7 pp to +10.0 pp).
These differences remain statistically inconclusive at pilot scale: all pairwise McNemar tests are non-significant ($n=70$, $p \geq 0.17$), and power is limited.
Substantial inter-system agreement (Fleiss' $\kappa = 0.638$) is consistent with a rule-layer anchoring effect, but should be interpreted as preliminary evidence pending unseen-case validation.
Immediate next steps are:
\begin{enumerate}[nosep]
    \item expand the benchmark to at least 100 dual-expert-labeled cases to
          achieve sufficient statistical power for McNemar testing ($\geq$0.8
          at the observed effect sizes);
    \item establish a clean held-out test set from new Ahli-2 annotations
          (post-2026-02-23) to validate generalization without development
          contamination;
    \item redesign router and prompt to address implicit conflict detection
          failures (C$\rightarrow$B errors) and enable D-label abstention;
    \item resolve the 4 remaining disputed cases via Delphi Round 2
          adjudication and document the rubric refinement process for
          methodological transparency.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{paper/references}

\end{document}
