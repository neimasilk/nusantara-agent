\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{array}

\title{Neuro-Symbolic Legal Reasoning with Expert-Verified Customary Law Rules: A Pilot Study on Indonesian Legal Pluralism}
\author{
Anonymous Authors\\
Project: Nusantara-Agent
}
\date{Draft v0.3 -- 2026-02-19}

\begin{document}
\maketitle

\begin{abstract}
We present \textit{Nusantara-Agent}, a neuro-symbolic framework for legal reasoning under Indonesian legal pluralism.
The core contribution is a formally encoded customary-law rule base in Answer Set Programming (ASP), constructed from expert-verified rules across three domains: Minangkabau (25 rules), Bali (34 rules), and Jawa (36 rules).
An LLM adjudication layer synthesizes symbolic outputs into final classifications.
On a benchmark of 74 dual-labeled legal scenarios (70 evaluable, 4 disputed), we compare ASP-only (keyword heuristic) against ASP+LLM (neuro-symbolic).
ASP-only achieves 58.6\% accuracy (41/70), while ASP+LLM achieves 70.0\% (49/70), Wilson 95\% CI [0.585, 0.795].
Per-label recall for ASP+LLM is A=5/6, B=24/31, C=20/31, and D=0/2.
Inter-rater agreement between two legal experts is 94.6\% on the expanded batch (50 new cases), up from 58.3\% on the initial batch after rubric refinement.
These findings demonstrate that expert-verified symbolic rules provide measurable value as an LLM reasoning anchor for plural-law classification.
\end{abstract}

\section{Introduction}
Plural legal reasoning in Indonesia requires reconciling national law and multiple customary law systems.
This setting is relevant for AI because legal outcomes depend on cross-regime conflict handling, not only document retrieval.
Our objective is to build an auditable framework that can:
\begin{enumerate}[nosep]
  \item formalize customary legal constraints,
  \item combine symbolic and neural reasoning under explicit traceability, and
  \item report disagreement and uncertainty instead of forcing brittle single-label outputs.
\end{enumerate}

\paragraph{Current scope.}
Following the 2026-02-12 pivot, this manuscript focuses on ASP rule formalization, expert verification, and pilot evaluation.
Claims about large-scale generalization are intentionally deferred.

\section{Related Work}
This work is connected to neuro-symbolic AI, legal-domain NLP, and LLM-based reasoning.
Foundational LLM and prompting work includes Transformers, few-shot scaling, chain-of-thought, and tool-using reasoning \cite{vaswani2017attention,brown2020language,wei2022cot,wang2022selfconsistency,yao2023react,yao2023tree,shinn2023reflexion}.
Recent retrieval-centered directions include classical RAG, self-reflective RAG, and hierarchical retrieval designs \cite{lewis2020rag,asai2023selfrag,gao2023ragsurvey,sarthi2024raptor,he2024gretriever}.
For legal-domain NLP, we reference benchmark and applied studies in legal judgment prediction, contract IE, legal summarization, and deployment settings \cite{guha2023legalbench,colombo2024saullm,sie-etal-2024-summarizing,liu-etal-2024-enhancing-legal,nigam-etal-2024-rethinking,xie-etal-2024-clc,mali-etal-2024-information,narendra-etal-2024-enhancing,hou-etal-2024-gaps,kwak-etal-2024-classify,taranukhin-etal-2024-empowering,tran-etal-2024-deberta,tyss-etal-2024-lexsumm,tyss-etal-2024-supporting}.

\section{Task Definition}
Given a legal scenario $x$, the system predicts one of four policy labels:
\begin{itemize}[nosep]
  \item \textbf{A}: mainly national-law resolution,
  \item \textbf{B}: mainly customary-law resolution,
  \item \textbf{C}: synthesis of national + customary law,
  \item \textbf{D}: clarification required.
\end{itemize}

\noindent
Predictions are compared against expert labels with explicit agreement status.

\section{System Overview}
Nusantara-Agent is rule-centric: ASP formalization of expert-verified customary law is the primary modeling contribution.
The LLM-based adjudication layer is used as a practical decision interface and is not the central novelty claim.

\subsection{Core Neuro-Symbolic Components}
\begin{itemize}[nosep]
  \item Keyword router and fact extraction for domain routing and ASP fact grounding.
  \item ASP rule engine (Clingo) for hard legal constraints and contradiction signals.
  \item Optional LLM adjudication for final label synthesis; offline fallback for deterministic operation.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lrrl}
\toprule
Domain & Expert-Verified Rules & ASP Rule File & Scope \\
\midrule
Minangkabau & 25 & \texttt{minangkabau.lp} & Matrilineal inheritance and pusako norms \\
Bali & 34 & \texttt{bali.lp} & Purusa/sentana and druwe classifications \\
Jawa & 36 & \texttt{jawa.lp} & Bilateral inheritance and gono-gini patterns \\
\midrule
Total & 95 & 3 domains & Expert-verified customary law base \\
\bottomrule
\end{tabular}
\caption{Expert-verified customary-law rules encoded in ASP.}
\label{tab:rulebase}
\end{table}

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{m{2.6cm}c m{2.6cm}c m{2.6cm}c m{2.6cm}}
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Input\\Case Query}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Router +\\Fact Extraction}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering ASP Rule\\Engine}} &
$\rightarrow$ &
\fbox{\parbox[c][1.5cm][c]{2.4cm}{\centering Label Proposal\\(A/B/C/D)}}
\end{tabular}

\vspace{0.4em}
\begin{tabular}{m{3.6cm}c m{3.6cm}c m{3.6cm}}
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Optional LLM\\Adjudication}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Offline Safety Net\\(No API Mode)}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{3.3cm}{\centering Final Report\\+ Trace}}
\end{tabular}
\caption{High-level architecture of Nusantara-Agent.}
\label{fig:architecture}
\end{figure}

\section{Dataset and Expert Protocol}
\subsection{Current Snapshot (2026-02-19)}
The benchmark dataset was constructed in two phases: an initial batch of 24 cases and an expanded batch of 50 new cases.
Each case was independently labeled by two qualified legal experts (Ahli-1: Dr.~Hendra Kusuma, S.H., M.Hum.; Ahli-2: Dr.~Indra Gunawan, S.H., M.H.).

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Item & Value \\
\midrule
Total cases in benchmark & 74 \\
Evaluable agreed cases & 70 \\
Disputed cases (excluded from accuracy) & 4 \\
Initial batch agreement (24 cases) & 58.3\% (Cohen's $\kappa$ = 0.394) \\
Expanded batch agreement (50 cases) & 94.0\% (47/50) \\
Benchmark manifest source & \texttt{data/benchmark\_manifest.json} \\
\bottomrule
\end{tabular}
\caption{Operational dataset status.}
\label{tab:status}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
Gold Label & Count & Percentage \\
\midrule
A & 6 & 8.6\% \\
B & 31 & 44.3\% \\
C & 31 & 44.3\% \\
D & 2 & 2.9\% \\
\bottomrule
\end{tabular}
\caption{Label distribution in the evaluable benchmark subset (N=70).}
\label{tab:labeldist}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Agreement Metric & Value \\
\midrule
Initial batch agreed/total & 14/24 (58.3\%) \\
Expanded batch agreed/total & 47/50 (94.0\%) \\
Adjudicated (initial disputed) & 9/10 resolved \\
Remaining disputed & 4/74 (5.4\%) \\
Accuracy reporting policy & Computed only on agreed subset \\
\bottomrule
\end{tabular}
\caption{Agreement profile across annotation phases.}
\label{tab:consensus}
\end{table}

\subsection{Rubric refinement}
Initial inter-rater agreement was moderate ($\kappa$ = 0.394), primarily due to ambiguity at the B/C boundary (pure adat vs.\ national--adat conflict).
A structured rubric with explicit boundary guidance was developed and distributed before the expanded batch.
This improved agreement to 94\%, with all 3 remaining disputes occurring at the same B/C boundary in Jawa inheritance cases.

\subsection{Adjudication policy}
For the current evaluation:
\begin{itemize}[nosep]
  \item only agreed cases are used for quantitative accuracy reporting,
  \item disputed cases are excluded and queued for third-expert tiebreak,
  \item benchmark manifest is treated as the reporting authority for snapshot counts.
\end{itemize}

\section{Metrics and Equations}
To avoid ambiguous reporting, we explicitly define pilot metrics.

\subsection{Agreement Coverage}
Let $N$ be total active cases and $N_a$ the agreed/evaluable subset.
\begin{equation}
\mathrm{ConsensusStrength} = \frac{N_a}{N}
\label{eq:consensus_strength}
\end{equation}
For the current snapshot, $(N_a, N)=(70,74)$, so:
\begin{equation}
\mathrm{ConsensusStrength} = \frac{70}{74}=0.946
\end{equation}

\subsection{Dispute Rate}
Let $N_d$ be the number of disputed cases:
\begin{equation}
\mathrm{TieRate} = \frac{N_d}{N}
\label{eq:tierate}
\end{equation}
With $N_d=4$ and $N=74$, $\mathrm{TieRate}=0.054$.

\subsection{Binomial Confidence Interval (Wilson)}
For observed accuracy $\hat{p}$ with sample size $n$ and $z=1.96$ (95\% CI), Wilson interval is:
\begin{equation}
\frac{\hat{p} + \frac{z^2}{2n} \pm
z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{z^2}{4n^2}}}
{1+\frac{z^2}{n}}
\label{eq:wilson}
\end{equation}
This is used to report uncertainty on small pilot datasets and avoid over-claiming.

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{m{2.4cm}c m{2.4cm}c m{2.4cm}c m{2.4cm}}
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Freeze\\Dataset}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Run\\Benchmark}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Compute\\95\% CI}} &
$\rightarrow$ &
\fbox{\parbox[c][1.4cm][c]{2.2cm}{\centering Claim\\Gate}}
\end{tabular}
\caption{Evaluation protocol and claim gate used for pilot reporting.}
\label{fig:gate}
\end{figure}

\section{Experiments and Results}
\subsection{Ablation: ASP-only vs ASP+LLM (2026-02-19)}
We compare two operating modes on the 70 evaluable cases:
\begin{itemize}[nosep]
  \item \textbf{ASP-only}: keyword router $\rightarrow$ ASP rule engine $\rightarrow$ offline heuristic supervisor (no LLM calls).
  \item \textbf{ASP+LLM}: keyword router $\rightarrow$ ASP rule engine $\rightarrow$ LLM adjudication via Qwen-2.5-7B-Instruct (local Ollama).
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Mode & Accuracy & Wilson 95\% CI & Correct / Total \\
\midrule
ASP-only (offline) & 58.6\% & [0.469, 0.694] & 41 / 70 \\
ASP+LLM (Qwen-2.5-7B) & \textbf{70.0\%} & [0.585, 0.795] & 49 / 70 \\
\bottomrule
\end{tabular}
\caption{Ablation comparison on the 70-case evaluable benchmark.}
\label{tab:ablation}
\end{table}

The LLM layer improves accuracy by 11.4 percentage points, confirming that the neuro-symbolic combination outperforms the symbolic-only baseline.

\begin{table}[h]
\centering
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{ASP-only} & \multicolumn{3}{c}{ASP+LLM} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Label & P & R & Support & P & R & Support \\
\midrule
A & 0.36 & 0.67 & 6 & 0.36 & 0.83 & 6 \\
B & 0.66 & 0.61 & 31 & 0.75 & 0.77 & 31 \\
C & 0.62 & 0.58 & 31 & 0.83 & 0.65 & 31 \\
D & 0.00 & 0.00 & 2 & 0.00 & 0.00 & 2 \\
\bottomrule
\end{tabular}
\caption{Per-label precision (P) and recall (R) for both modes on the 70-case benchmark.}
\label{tab:labelperf}
\end{table}

\subsection{Error analysis}
The 21 errors in ASP+LLM cluster into three patterns:
\begin{enumerate}[nosep]
  \item \textbf{C$\rightarrow$B misclassification} (11 cases): conflict cases where the national-law dimension is implicit rather than keyword-detectable; the keyword router does not flag the conflict, so the LLM adjudicator defaults to pure-adat.
  \item \textbf{B$\rightarrow$A/C false positives} (8 cases): pure-adat definitional passages containing incidental national-law terms (e.g., ``KUHPerdata'', ``pasal'') that trigger false conflict signals.
  \item \textbf{D-label failure} (2 cases): the system consistently fails to predict ``insufficient information''; it always assigns a substantive label.
\end{enumerate}

\subsection{What is intentionally deferred}
\begin{itemize}[nosep]
  \item McNemar test for statistical significance of ASP-only vs ASP+LLM difference,
  \item cross-validation with alternative LLMs (DeepSeek, Kimi) for robustness check,
  \item expansion to 100+ cases for narrower confidence intervals.
\end{itemize}

\section{Threats to Validity and Limitations}
\begin{enumerate}[nosep]
  \item \textbf{Sample size}: 70 evaluable cases provide moderate statistical power; Wilson CI width is $\sim$0.21.
  \item \textbf{Label imbalance}: A (8.6\%) and D (2.9\%) are underrepresented; per-label metrics for these classes have high variance.
  \item \textbf{D-label failure}: the system fails on both D cases (0/2 recall), indicating absent uncertainty detection capability.
  \item \textbf{Single LLM}: results reported for Qwen-2.5-7B only; robustness across LLM families is not yet established.
  \item \textbf{ASP coverage gaps}: 24 expert-verified rules still lack ASP encodings (Min: 8, Bali: 9, Jawa: 7).
  \item \textbf{B/C boundary}: the most common error pattern (C$\rightarrow$B) reflects difficulty in detecting implicit national--adat conflict from keyword routing alone.
\end{enumerate}

\section{Data and Code Availability}
The dataset and source code are publicly available at:
\begin{center}
\url{https://github.com/neimasilk/nusantara-agent.git}
\end{center}

\section{Conclusion and Next Steps}
We demonstrate that expert-verified ASP rules combined with LLM adjudication achieve 70.0\% accuracy on a 70-case plural-law benchmark, an 11.4-point improvement over ASP-only keyword heuristics.
The result confirms that symbolic customary-law encoding provides measurable value as a reasoning anchor for LLM-based legal classification in Indonesia.
Immediate next steps are:
\begin{enumerate}[nosep]
  \item cross-validate with alternative LLMs (DeepSeek, Kimi) and run McNemar significance tests,
  \item close the remaining 24 ASP rule encoding gaps across all three domains,
  \item expand the benchmark to 100+ cases and resolve the 4 remaining disputed cases.
\end{enumerate}

\nocite{*}
\bibliographystyle{plain}
\bibliography{paper/references}

\end{document}
