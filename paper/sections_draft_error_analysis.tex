% Draft Sections: Error Analysis and Discussion
% For Nusantara-Agent Paper
% Date: 2026-02-19

% =============================================================================
% SECTION: Extended Error Analysis (replaces subsection in main.tex)
% =============================================================================

\subsection{Extended Error Analysis: The 12 Hard Cases}

Beyond the aggregate error patterns described above, we conduct a deeper analysis of \textbf{12 cases (17.1\%)} where all three system variants---ASP-only, ASP+Ollama, and ASP+DeepSeek---fail simultaneously. These ``hard cases'' reveal systematic limitations in the current rule set and router design.

\subsubsection{Gold label distribution in hard cases}

Table~\ref{tab:hardcases-dist} shows that conflict cases (C) are over-represented in failures, comprising 50.0\% of hard cases versus 44.3\% overall. This confirms that detecting implicit national--adat conflicts remains the primary challenge.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Gold Label & Hard Cases & Overall & Ratio \\
\midrule
C (Conflict) & 6 (50.0\%) & 31 (44.3\%) & 1.13$\times$ \\
B (Adat) & 5 (41.7\%) & 31 (44.3\%) & 0.94$\times$ \\
A (National) & 1 (8.3\%) & 6 (8.6\%) & 0.97$\times$ \\
D (Unclear) & 0 (0.0\%) & 2 (2.9\%) & 0.00$\times$ \\
\bottomrule
\end{tabular}
\caption{Label distribution in 12 hard cases versus overall benchmark (N=70).}
\label{tab:hardcases-dist}
\end{table}

\subsubsection{Dominant failure patterns}

Table~\ref{tab:failure-patterns} categorizes the failure modes. The dominant pattern is \textbf{C$\rightarrow$B misclassification} (33.3\% of hard cases): conflicts where ethnic keywords dominate the query text, causing the symbolic router to overwhelm conflict signals.

\begin{table}[h]
\centering
\begin{tabular}{llcc}
\toprule
Pattern & Description & Count & Percentage \\
\midrule
C$\rightarrow$B & Conflict misclassified as Adat & 4 & 33.3\% \\
B$\rightarrow$A & Adat misclassified as National & 3 & 25.0\% \\
B$\rightarrow$C & Adat misclassified as Conflict & 2 & 16.7\% \\
C$\rightarrow$A & Conflict misclassified as National & 2 & 16.7\% \\
A$\rightarrow$C & National misclassified as Conflict & 1 & 8.3\% \\
\bottomrule
\end{tabular}
\caption{Failure patterns in 12 hard cases where all three systems fail.}
\label{tab:failure-patterns}
\end{table}

The prevalence of C$\rightarrow$B errors suggests that when customary-law keywords (e.g., ``Minangkabau,'' ``pusako,'' ``gono-gini'') appear prominently, the keyword router defaults to the pure-adat pathway even when national-law dimensions are implicitly present. This is particularly evident in Minangkabau cases involving joint property (\textit{harta bersama}), where ownership disputes span both customary inheritance norms and national civil code provisions.

\subsubsection{Domain-specific challenges}

Minangkabau cases are over-represented in failures (33.3\% of hard cases), particularly around:
\begin{itemize}[nosep]
  \item \textit{Harta pusako} (inherited property) classification,
  \item Inter-ethnic marriage property regimes,
  \item Joint business property ownership disputes.
\end{itemize}

This reflects the complexity of matrilineal inheritance rules and their intersection with national civil law, which the current ASP rule base captures incompletely.

\subsubsection{D-label complete failure}

All three systems fail on both D (``clarification required'') cases in the benchmark (0/2 recall). The system consistently assigns substantive labels (A, B, or C) even when the scenario contains insufficient information for legal determination. This indicates that \textbf{uncertainty detection} is not supported by the current rule set or prompt design---a critical gap for real-world deployment where refusing to classify is often the correct response.

\subsubsection{Root cause summary}

The fact that all three systems fail identically on these 12 cases indicates the problem lies in the \textbf{shared ASP rule set} or \textbf{router/prompt design}, not in LLM backend choice. The primary failure modes are:
\begin{enumerate}[nosep]
  \item \textbf{Implicit conflict detection}: When conflict keywords are subtle (e.g., ``mendahului,'' ``tidak sah'') and ethnic keywords dominate, the router misses the conflict signal.
  \item \textbf{Domain ambiguity}: ``General'' cases without clear domain markers default to national-law pathways, missing adat dimensions.
  \item \textbf{No abstention mechanism}: The system cannot output D even when confidence is low.
\end{enumerate}

% =============================================================================
% SECTION: Cross-Validation Discussion
% =============================================================================

\section{Cross-Validation Across LLM Backends}

To assess whether results are robust to LLM choice, we evaluate three operating configurations on the identical 70-case benchmark:
\begin{itemize}[nosep]
  \item \textbf{ASP-only}: offline heuristic supervisor (no LLM calls);
  \item \textbf{ASP+Ollama}: Qwen-2.5-7B-Instruct via local Ollama;
  \item \textbf{ASP+DeepSeek}: DeepSeek-Chat via API.
\end{itemize}

\subsection{Accuracy comparison}

Table~\ref{tab:crossval} reports accuracy and agreement metrics. DeepSeek achieves the highest accuracy at 67.1\%, followed by Ollama at 62.9\%, with ASP-only at 58.6\%. All three systems achieve at least ``Fair'' agreement with the gold standard (Cohen's $\kappa \geq 0.331$), with DeepSeek reaching ``Moderate'' agreement ($\kappa = 0.473$).

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Mode & Accuracy & Wilson 95\% CI & Cohen's $\kappa$ & Agreement \\
\midrule
ASP-only & 58.6\% & [0.469, 0.694] & 0.331 & Fair \\
ASP+Ollama & 62.9\% & [0.512, 0.733] & 0.411 & Moderate \\
ASP+DeepSeek & \textbf{67.1\%} & [0.555, 0.770] & \textbf{0.473} & Moderate \\
\bottomrule
\end{tabular}
\caption{Cross-validation across three LLM backends (N=70 evaluable cases).}
\label{tab:crossval}
\end{table}

\subsection{Statistical significance}

Table~\ref{tab:mcnemar} presents pairwise McNemar tests. \textbf{None of the differences reach statistical significance} ($p > 0.05$ for all pairs), likely due to limited sample size ($n=70$). The largest observed difference is ASP-only vs.~DeepSeek (6 discordant pairs favoring DeepSeek), with $p = 0.238$.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Comparison & $p$-value & Discordant & Favored \\
\midrule
ASP-only vs.~Ollama & 0.508 & 9 & Ollama (+3) \\
ASP-only vs.~DeepSeek & 0.238 & 18 & DeepSeek (+6) \\
Ollama vs.~DeepSeek & 0.549 & 11 & DeepSeek (+3) \\
\bottomrule
\end{tabular}
\caption{McNemar test results for pairwise accuracy differences.}
\label{tab:mcnemar}
\end{table}

\subsection{Inter-system agreement}

Despite using different LLM backends, the three systems exhibit \textbf{substantial agreement} with each other. Fleiss' $\kappa = 0.623$ indicates that the ASP+LLM architecture produces consistent predictions regardless of the specific LLM used. Table~\ref{tab:agreement} breaks down cross-model agreement patterns.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Agreement Pattern & Count & Percentage \\
\midrule
All 3 agree (correct) & 34 & 48.6\% \\
All 3 agree (wrong) & 12 & 17.1\% \\
\midrule
\textbf{All 3 agree (total)} & \textbf{46} & \textbf{65.7\%} \\
2 agree, 1 different & 22 & 31.4\% \\
All 3 different & 2 & 2.9\% \\
\bottomrule
\end{tabular}
\caption{Cross-model agreement among ASP-only, Ollama, and DeepSeek.}
\label{tab:agreement}
\end{table}

The high unanimous agreement (65.7\%) with only 2.9\% complete disagreement suggests that the symbolic rule layer acts as a \textbf{strong prior} that dominates LLM variation. This is a desirable property for reproducibility: the system's behavior is anchored by the expert-verified ASP rules rather than being sensitive to LLM choice.

\subsection{Majority vote analysis}

For the 22 cases where two systems agree and one differs, majority vote achieves only \textbf{50.0\% accuracy} (11/22). This indicates that when systems disagree, the majority is \textbf{no more likely to be correct than random chance}. Consequently, simple ensemble strategies offer no improvement over single-system predictions in this setting.

\subsection{Implications}

The cross-validation results support two key claims:
\begin{enumerate}[nosep]
  \item \textbf{Reproducibility}: A local 7B parameter model (Qwen-2.5-7B) achieves performance competitive with a commercial API (DeepSeek), with only a 4.2 percentage point gap that is not statistically significant. This supports deployment in resource-constrained settings.
  \item \textbf{Rule-centric design}: The high inter-system agreement ($\kappa = 0.623$) confirms that the ASP rule layer, not the LLM backend, is the primary determinant of system behavior. This validates our design choice of treating the LLM as an adjudication interface rather than the core reasoning engine.
\end{enumerate}

% =============================================================================
% SECTION: Limitations (expands existing section)
% =============================================================================

\section{Expanded Limitations}

We expand the limitations section in the main paper with findings from cross-validation and hard-case analysis:

\begin{enumerate}[nosep]
  \item \textbf{Sample size}: $n=70$ evaluable cases provides insufficient statistical power to detect differences between LLM backends; McNemar $p$-values are all $>0.05$.
  
  \item \textbf{Expert agreement}: Only two expert raters with initial Cohen's $\kappa = 0.394$ (fair agreement), though this improved to 94\% after rubric refinement.
  
  \item \textbf{Class imbalance}: Label D has only 2 cases (2.9\%), resulting in 0\% recall across all systems. This extreme imbalance precludes meaningful evaluation of uncertainty detection.
  
  \item \textbf{Rule verbosity trade-off}: Adding more ASP rules may not improve LLM performance if the router cannot effectively map query text to the correct rules. The 12 hard cases suggest rule--router alignment is the bottleneck, not rule coverage alone.
  
  \item \textbf{Single-language evaluation}: All cases are in Indonesian; generalization to other legal pluralism contexts (e.g., multilingual settings, other customary law systems) is not established.
  
  \item \textbf{Static benchmark}: The benchmark is frozen at 74 cases with 4 disputed; expansion to 100+ cases is needed for narrower confidence intervals.
\end{enumerate}

% =============================================================================
% END OF DRAFT
% =============================================================================
